{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91959\\Desktop\\Personal\\projects\\transformers\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "from pprint import pprint\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import numpy as np #required implicitly for training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "# cam also use amazon_polarity dataset for sentiment analysis but takes a long time to process\n",
    "# raw_datasets = load_dataset(\"amazon_polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label', 'idx'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_TF_DATASET_REFS',\n",
       " '__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getitems__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_build_local_temp_path',\n",
       " '_check_index_is_initialized',\n",
       " '_data',\n",
       " '_estimate_nbytes',\n",
       " '_fingerprint',\n",
       " '_format_columns',\n",
       " '_format_kwargs',\n",
       " '_format_type',\n",
       " '_generate_tables_from_cache_file',\n",
       " '_generate_tables_from_shards',\n",
       " '_get_cache_file_path',\n",
       " '_get_output_signature',\n",
       " '_getitem',\n",
       " '_indexes',\n",
       " '_indices',\n",
       " '_info',\n",
       " '_map_single',\n",
       " '_new_dataset_with_indices',\n",
       " '_output_all_columns',\n",
       " '_push_parquet_shards_to_hub',\n",
       " '_save_to_disk_single',\n",
       " '_select_contiguous',\n",
       " '_select_with_indices_mapping',\n",
       " '_split',\n",
       " 'add_column',\n",
       " 'add_elasticsearch_index',\n",
       " 'add_faiss_index',\n",
       " 'add_faiss_index_from_external_arrays',\n",
       " 'add_item',\n",
       " 'align_labels_with_mapping',\n",
       " 'builder_name',\n",
       " 'cache_files',\n",
       " 'cast',\n",
       " 'cast_column',\n",
       " 'citation',\n",
       " 'class_encode_column',\n",
       " 'cleanup_cache_files',\n",
       " 'column_names',\n",
       " 'config_name',\n",
       " 'data',\n",
       " 'dataset_size',\n",
       " 'description',\n",
       " 'download_checksums',\n",
       " 'download_size',\n",
       " 'drop_index',\n",
       " 'export',\n",
       " 'features',\n",
       " 'filter',\n",
       " 'flatten',\n",
       " 'flatten_indices',\n",
       " 'format',\n",
       " 'formatted_as',\n",
       " 'from_buffer',\n",
       " 'from_csv',\n",
       " 'from_dict',\n",
       " 'from_file',\n",
       " 'from_generator',\n",
       " 'from_json',\n",
       " 'from_list',\n",
       " 'from_pandas',\n",
       " 'from_parquet',\n",
       " 'from_spark',\n",
       " 'from_sql',\n",
       " 'from_text',\n",
       " 'get_index',\n",
       " 'get_nearest_examples',\n",
       " 'get_nearest_examples_batch',\n",
       " 'homepage',\n",
       " 'info',\n",
       " 'is_index_initialized',\n",
       " 'iter',\n",
       " 'license',\n",
       " 'list_indexes',\n",
       " 'load_elasticsearch_index',\n",
       " 'load_faiss_index',\n",
       " 'load_from_disk',\n",
       " 'map',\n",
       " 'num_columns',\n",
       " 'num_rows',\n",
       " 'prepare_for_task',\n",
       " 'push_to_hub',\n",
       " 'remove_columns',\n",
       " 'rename_column',\n",
       " 'rename_columns',\n",
       " 'reset_format',\n",
       " 'save_faiss_index',\n",
       " 'save_to_disk',\n",
       " 'search',\n",
       " 'search_batch',\n",
       " 'select',\n",
       " 'select_columns',\n",
       " 'set_format',\n",
       " 'set_transform',\n",
       " 'shape',\n",
       " 'shard',\n",
       " 'shuffle',\n",
       " 'size_in_bytes',\n",
       " 'sort',\n",
       " 'split',\n",
       " 'supervised_keys',\n",
       " 'task_templates',\n",
       " 'to_csv',\n",
       " 'to_dict',\n",
       " 'to_iterable_dataset',\n",
       " 'to_json',\n",
       " 'to_list',\n",
       " 'to_pandas',\n",
       " 'to_parquet',\n",
       " 'to_sql',\n",
       " 'to_tf_dataset',\n",
       " 'train_test_split',\n",
       " 'unique',\n",
       " 'version',\n",
       " 'with_format',\n",
       " 'with_transform']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(raw_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "sentence: string\n",
       "label: int64\n",
       "idx: int32\n",
       "----\n",
       "sentence: [[\"hide new secretions from the parental units \",\"contains no wit , only labored gags \",\"that loves its characters and communicates something rather beautiful about human nature \",\"remains utterly satisfied to remain the same throughout \",\"on the worst revenge-of-the-nerds clich√©s the filmmakers could dredge up \",...,\"you wish you were at home watching that movie instead of in the theater watching this one \",\"'s no point in extracting the bare bones of byatt 's plot for purposes of bland hollywood romance \",\"underdeveloped \",\"the jokes are flat \",\"a heartening tale of small victories \"],[\"suspense , intriguing characters and bizarre bank robberies , \",\"a gritty police thriller with all the dysfunctional family dynamics one could wish for \",\"with a wonderful ensemble cast of characters that bring the routine day to day struggles of the working class to life \",\"nonetheless appreciates the art and reveals a music scene that transcends culture and race . \",\"do we really need the tiger beat version ? \",...,\"when there 's nothing else happening \",\"on cable \",\"it with ring , \",\"far from a groundbreaking endeavor \",\"that these women are spectacular \"],...,[\"it does turn out to be a bit of a cheat in the end \",\"may be convinced that he has something significant to say \",\"to be both hugely entertaining and uplifting . \",\", boredom never takes hold . \",\"left to work with , sort of like michael jackson 's nose \",...,\"from a severe case of hollywood-itis \",\"the very best of them \",\"thrills , \",\"'s attracting audiences to unfaithful \",\"impressively delicate range \"],[\"starts off promisingly but then proceeds to flop \",\"distinguished actor \",\"on their parents ' anguish \",\"pays off and is effective if you stick with it \",\"is n't particularly funny \",...,\"a delightful comedy \",\"anguish , anger and frustration \",\"at achieving the modest , crowd-pleasing goals it sets for itself \",\"a patient viewer \",\"this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \"]]\n",
       "label: [[0,0,1,0,0,...,0,0,0,0,1],[1,1,1,1,0,...,0,0,1,0,1],...,[0,0,1,1,0,...,0,1,1,1,1],[0,1,0,1,0,...,1,0,1,1,0]]\n",
       "idx: [[0,1,2,3,4,...,995,996,997,998,999],[1000,1001,1002,1003,1004,...,1995,1996,1997,1998,1999],...,[66000,66001,66002,66003,66004,...,66995,66996,66997,66998,66999],[67000,67001,67002,67003,67004,...,67344,67345,67346,67347,67348]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray object at 0x000001789E0E8220>\n",
       "[\n",
       "  [\n",
       "    \"hide new secretions from the parental units \",\n",
       "    \"contains no wit , only labored gags \",\n",
       "    \"that loves its characters and communicates something rather beautiful about human nature \",\n",
       "    \"remains utterly satisfied to remain the same throughout \",\n",
       "    \"on the worst revenge-of-the-nerds clich√©s the filmmakers could dredge up \",\n",
       "    ...\n",
       "    \"you wish you were at home watching that movie instead of in the theater watching this one \",\n",
       "    \"'s no point in extracting the bare bones of byatt 's plot for purposes of bland hollywood romance \",\n",
       "    \"underdeveloped \",\n",
       "    \"the jokes are flat \",\n",
       "    \"a heartening tale of small victories \"\n",
       "  ],\n",
       "  [\n",
       "    \"suspense , intriguing characters and bizarre bank robberies , \",\n",
       "    \"a gritty police thriller with all the dysfunctional family dynamics one could wish for \",\n",
       "    \"with a wonderful ensemble cast of characters that bring the routine day to day struggles of the working class to life \",\n",
       "    \"nonetheless appreciates the art and reveals a music scene that transcends culture and race . \",\n",
       "    \"do we really need the tiger beat version ? \",\n",
       "    ...\n",
       "    \"when there 's nothing else happening \",\n",
       "    \"on cable \",\n",
       "    \"it with ring , \",\n",
       "    \"far from a groundbreaking endeavor \",\n",
       "    \"that these women are spectacular \"\n",
       "  ],\n",
       "...,\n",
       "  [\n",
       "    \"it does turn out to be a bit of a cheat in the end \",\n",
       "    \"may be convinced that he has something significant to say \",\n",
       "    \"to be both hugely entertaining and uplifting . \",\n",
       "    \", boredom never takes hold . \",\n",
       "    \"left to work with , sort of like michael jackson 's nose \",\n",
       "    ...\n",
       "    \"from a severe case of hollywood-itis \",\n",
       "    \"the very best of them \",\n",
       "    \"thrills , \",\n",
       "    \"'s attracting audiences to unfaithful \",\n",
       "    \"impressively delicate range \"\n",
       "  ],\n",
       "  [\n",
       "    \"starts off promisingly but then proceeds to flop \",\n",
       "    \"distinguished actor \",\n",
       "    \"on their parents ' anguish \",\n",
       "    \"pays off and is effective if you stick with it \",\n",
       "    \"is n't particularly funny \",\n",
       "    ...\n",
       "    \"a delightful comedy \",\n",
       "    \"anguish , anger and frustration \",\n",
       "    \"at achieving the modest , crowd-pleasing goals it sets for itself \",\n",
       "    \"a patient viewer \",\n",
       "    \"this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \"\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['in memory ',\n",
       "  'respectable new one ',\n",
       "  'yet this grating showcase ',\n",
       "  'hate to tear your eyes away from the images long enough to read the subtitles ',\n",
       "  'addition to sporting one of the worst titles in recent cinematic history '],\n",
       " 'label': [1, 1, 0, 1, 0],\n",
       " 'idx': [100, 101, 102, 103, 104]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#despite label names in the dataset, model by default outputs only generic labels\n",
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
      "               [101,\n",
      "                3397,\n",
      "                2053,\n",
      "                15966,\n",
      "                1010,\n",
      "                2069,\n",
      "                4450,\n",
      "                2098,\n",
      "                18201,\n",
      "                2015,\n",
      "                102],\n",
      "               [101,\n",
      "                2008,\n",
      "                7459,\n",
      "                2049,\n",
      "                3494,\n",
      "                1998,\n",
      "                10639,\n",
      "                2015,\n",
      "                2242,\n",
      "                2738,\n",
      "                3376,\n",
      "                2055,\n",
      "                2529,\n",
      "                3267,\n",
      "                102]]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = tokenizer(raw_datasets['train'][0:3]['sentence'])\n",
    "pprint(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1821/1821 [00:00<00:00, 24737.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='my_trainer',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "‚îú‚îÄDistilBertModel: 1-1                                  --\n",
       "‚îÇ    ‚îî‚îÄEmbeddings: 2-1                                  --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-1                              23,440,896\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄEmbedding: 3-2                              393,216\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-3                              1,536\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-4                                --\n",
       "‚îÇ    ‚îî‚îÄTransformer: 2-2                                 --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄModuleList: 3-5                             42,527,232\n",
       "‚îú‚îÄLinear: 1-2                                           590,592\n",
       "‚îú‚îÄLinear: 1-3                                           1,538\n",
       "‚îú‚îÄDropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check if the entire network gets trained or only head\n",
    "params_before = []\n",
    "for name, p in model.named_parameters(): \n",
    "    params_before.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91959\\AppData\\Local\\Temp\\ipykernel_22732\\2347420749.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"sst2\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"glue\", \"sst2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[1, 0, 1], references=[1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the compute_metrics function:\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions = predictions, references=labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8419 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 502/8419 [00:25<06:23, 20.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4188, 'learning_rate': 4.7030526190759e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 1003/8419 [00:50<06:08, 20.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3351, 'learning_rate': 4.4061052381518e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 1504/8419 [01:16<05:52, 19.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3495, 'learning_rate': 4.109157857227699e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 2003/8419 [01:44<05:45, 18.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2975, 'learning_rate': 3.812210476303599e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñâ       | 2502/8419 [02:11<05:14, 18.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.306, 'learning_rate': 3.515263095379499e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 3003/8419 [02:38<04:45, 18.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2901, 'learning_rate': 3.218315714455399e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 3503/8419 [03:07<04:31, 18.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2715, 'learning_rate': 2.9213683335312986e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 4002/8419 [03:34<03:55, 18.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2632, 'learning_rate': 2.6244209526071984e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4504/8419 [04:02<03:27, 18.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.232, 'learning_rate': 2.3274735716830978e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 5001/8419 [04:29<02:57, 19.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2483, 'learning_rate': 2.0305261907589976e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 5503/8419 [05:00<03:03, 15.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2153, 'learning_rate': 1.7335788098348973e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 6001/8419 [05:28<03:09, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2187, 'learning_rate': 1.4366314289107971e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 6504/8419 [05:58<01:39, 19.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2213, 'learning_rate': 1.1396840479866969e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 7003/8419 [06:25<01:14, 18.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2252, 'learning_rate': 8.427366670625965e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 7502/8419 [06:52<00:49, 18.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1965, 'learning_rate': 5.457892861384962e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 8003/8419 [07:20<00:21, 19.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2151, 'learning_rate': 2.4884190521439603e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8419/8419 [07:46<00:00, 17.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33744385838508606, 'eval_accuracy': 0.9071100917431193, 'eval_runtime': 1.5613, 'eval_samples_per_second': 558.514, 'eval_steps_per_second': 69.814, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8419/8419 [07:47<00:00, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 467.5955, 'train_samples_per_second': 144.033, 'train_steps_per_second': 18.005, 'train_loss': 0.26698761856291153, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8419, training_loss=0.26698761856291153, metrics={'train_runtime': 467.5955, 'train_samples_per_second': 144.033, 'train_steps_per_second': 18.005, 'train_loss': 0.26698761856291153, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import pipeline\n",
    "import json\n",
    "#adding id2label in the config file to output label names\n",
    "config_path = 'model/config.json'\n",
    "with open(config_path) as f:\n",
    "    j = json.load(f)\n",
    "    j['id2label'] = {0: 'negative', 1: 'positive'}\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(j, f, indent=2) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "saved_model = pipeline('text-classification', model='model', device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9989526271820068}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model(\"This movie is great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9987744688987732}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model(\"This movie is bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if whole network was changed:\n",
    "params_after = []\n",
    "for name, p in model.named_parameters():\n",
    "    params_after.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13252.66\n",
      "89.6709\n",
      "1.7980859\n",
      "1.075331\n",
      "1296.6256\n",
      "1.6503868\n",
      "1291.849\n",
      "0.0032623466\n",
      "1186.1066\n",
      "1.0579159\n",
      "1130.8407\n",
      "0.85436\n",
      "1.6720567\n",
      "0.84280396\n",
      "4925.7744\n",
      "5.775226\n",
      "4513.674\n",
      "0.7013656\n",
      "1.5714055\n",
      "0.6868653\n",
      "1263.667\n",
      "1.4004593\n",
      "1265.7064\n",
      "0.0029367977\n",
      "1107.6168\n",
      "0.83688736\n",
      "1068.6273\n",
      "0.7181506\n",
      "1.5539249\n",
      "0.69827795\n",
      "4899.924\n",
      "5.380926\n",
      "4464.1177\n",
      "0.6820636\n",
      "1.5906854\n",
      "0.6869984\n",
      "1272.8352\n",
      "1.5795727\n",
      "1277.6963\n",
      "0.0026745007\n",
      "1109.5337\n",
      "0.7855022\n",
      "1103.0457\n",
      "0.75032735\n",
      "1.5836492\n",
      "0.78737414\n",
      "4920.2007\n",
      "5.5639796\n",
      "4354.1943\n",
      "0.7123836\n",
      "1.4875911\n",
      "0.6933131\n",
      "1287.8937\n",
      "1.3822637\n",
      "1305.9032\n",
      "0.0032039424\n",
      "1160.582\n",
      "0.7359677\n",
      "1120.4573\n",
      "0.73621786\n",
      "1.4135668\n",
      "0.75952303\n",
      "4815.534\n",
      "5.4664145\n",
      "4138.236\n",
      "0.71775466\n",
      "1.3287185\n",
      "0.772182\n",
      "1211.8398\n",
      "1.4855764\n",
      "1206.5618\n",
      "0.0019893926\n",
      "991.55084\n",
      "0.8008404\n",
      "1006.32166\n",
      "0.8247078\n",
      "1.3721833\n",
      "0.9095831\n",
      "4411.2153\n",
      "5.108971\n",
      "3482.849\n",
      "0.697425\n",
      "1.3233086\n",
      "0.6415199\n",
      "1167.1584\n",
      "1.4287767\n",
      "1151.6066\n",
      "0.0012809257\n",
      "925.4199\n",
      "0.7206658\n",
      "903.3681\n",
      "0.9462185\n",
      "1.243445\n",
      "1.0946717\n",
      "3436.3528\n",
      "4.4317074\n",
      "3129.0574\n",
      "0.9437357\n",
      "1.2795303\n",
      "0.7156974\n",
      "857.47766\n",
      "0.92765844\n",
      "2.1562576\n",
      "0.0013325191\n"
     ]
    }
   ],
   "source": [
    "for p1, p2 in zip(params_before, params_after):\n",
    "    print(np.sum(np.abs(p1 - p2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
