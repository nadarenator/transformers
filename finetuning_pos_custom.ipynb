{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91959\\Desktop\\Personal\\projects\\transformers\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown \n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\91959\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\91959\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "corpus = brown.tagged_sents(tagset='universal')\n",
    "corpus #output is list of lists, with tuples of token, tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADV': 0,\n",
       " 'CONJ': 1,\n",
       " 'VERB': 2,\n",
       " 'ADJ': 3,\n",
       " 'ADP': 4,\n",
       " 'PRON': 5,\n",
       " 'X': 6,\n",
       " 'NUM': 7,\n",
       " '.': 8,\n",
       " 'PRT': 9,\n",
       " 'NOUN': 10,\n",
       " 'DET': 11}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tag2int dictionary\n",
    "tags = {tag for sentence in corpus for _, tag in sentence}\n",
    "tag2int = {tag: i for i, tag in enumerate(tags)}\n",
    "tag2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ADV',\n",
       " 1: 'CONJ',\n",
       " 2: 'VERB',\n",
       " 3: 'ADJ',\n",
       " 4: 'ADP',\n",
       " 5: 'PRON',\n",
       " 6: 'X',\n",
       " 7: 'NUM',\n",
       " 8: '.',\n",
       " 9: 'PRT',\n",
       " 10: 'NOUN',\n",
       " 11: 'DET'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#int2tag dictionary\n",
    "int2tag = {value:key for key, value in tag2int.items()}\n",
    "int2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to json\n",
    "converted_data = [{\n",
    "    \"inputs\": [token for token, _ in sentence],\n",
    "    \"targets\": [tag2int[tag] for _, tag in sentence]\n",
    "} for sentence in corpus]\n",
    "\n",
    "filename = './datasets/brown.json'\n",
    "\n",
    "# Use 'with' statement to open file to ensure proper closure after writing\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(converted_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 57340 examples [00:00, 70254.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 57340\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(path='json', data_files='./datasets/brown.json')\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 40138\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 17202\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train test split\n",
    "datasets = data['train'].train_test_split(test_size=0.3, seed=42)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casing is better for pos(Bill vs bill)\n",
    "checkpoint = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for aligning targets:\n",
    "def align_targets(labels, word_ids):\n",
    "    aligned_labels = []\n",
    "    last_id = None\n",
    "    for id in word_ids:\n",
    "        if id == None:\n",
    "            label = -100 #transformers(pytorch crossentropy loss) uses -100 for depicting targets that should not affect loss function \n",
    "        else:\n",
    "            label = labels[id] \n",
    "        aligned_labels.append(label)\n",
    "    return aligned_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "Co\tNOUN\n",
      "##st\tNOUN\n",
      "of\tADP\n",
      "power\tNOUN\n",
      "and\tCONJ\n",
      "machinery\tNOUN\n",
      "is\tVERB\n",
      "often\tADV\n",
      "a\tDET\n",
      "serious\tADJ\n",
      "problem\tNOUN\n",
      "to\tADP\n",
      "the\tDET\n",
      "small\tNOUN\n",
      "-\tNOUN\n",
      "scale\tNOUN\n",
      "farmer\tNOUN\n",
      ".\t.\n",
      "[SEP]\tNone\n"
     ]
    }
   ],
   "source": [
    "#checking target alignment:\n",
    "labels = datasets['train'][0]['targets']\n",
    "t = tokenizer(datasets['train'][0]['inputs'], is_split_into_words=True)\n",
    "word_ids = t.word_ids()\n",
    "aligned_targets = align_targets(labels, word_ids)\n",
    "aligned_labels =  [int2tag[t] if t>=0 else None for t in aligned_targets]\n",
    "for x, y in zip(t.tokens(), aligned_labels):\n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost\tNOUN\n",
      "of\tADP\n",
      "power\tNOUN\n",
      "and\tCONJ\n",
      "machinery\tNOUN\n",
      "is\tVERB\n",
      "often\tADV\n",
      "a\tDET\n",
      "serious\tADJ\n",
      "problem\tNOUN\n",
      "to\tADP\n",
      "the\tDET\n",
      "small-scale\tNOUN\n",
      "farmer\tNOUN\n",
      ".\t.\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(datasets['train'][0]['inputs'], datasets['train'][0]['targets']):\n",
    "    print(f\"{x}\\t{int2tag[y]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize fn:\n",
    "def tokenize_fn(batch):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch['inputs'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    labels_batch = batch['targets']\n",
    "    aligned_labels_batch = []\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids))\n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inputs', 'targets']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40138/40138 [00:04<00:00, 9890.78 examples/s] \n",
      "Map: 100%|██████████| 17202/17202 [00:01<00:00, 9765.89 examples/s] \n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(\n",
    "    function=tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=datasets['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 40138\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 17202\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3291,  2050,  1104,  1540,  1105, 11360,  1110,  1510,   170,\n",
       "          3021,  2463,  1106,  1103,  1353,   118,  3418,  9230,   119,   102],\n",
       "        [  101, 17323, 17941,   136,   136,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,   10,   10,    4,   10,    1,   10,    2,    0,   11,    3,   10,\n",
       "            4,   11,   10,   10,   10,   10,    8, -100],\n",
       "        [-100,    2,   10,    8,    8, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using data collator explicitly:\n",
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "batch #even padding tokens have -100 as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 2.0},\n",
       " '1': {'precision': 0.5,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 1.0},\n",
       " 'accuracy': 0.6666666666666666,\n",
       " 'macro avg': {'precision': 0.75,\n",
       "  'recall': 0.75,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 3.0},\n",
       " 'weighted avg': {'precision': 0.8333333333333334,\n",
       "  'recall': 0.6666666666666666,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 3.0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying out sklearn classification_report\n",
    "classification_report(\n",
    "    y_true=[0, 0, 1],\n",
    "    y_pred=[0, 1, 1],\n",
    "    output_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return [item for row in list_of_lists for item in row]\n",
    "\n",
    "flatten([[1,2,3], [4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    labels_jagged = [[l for l in row if l != -100] for row in labels]\n",
    "    predictions_jagged = [[p for (p,l) in zip(ps, ls) if l !=-100] for ps, ls in zip(predictions, labels)]\n",
    "    labels_flat = [label for row in labels_jagged for label in row]\n",
    "    predictions_flat = [prediction for row in predictions_jagged for prediction in row]\n",
    "    return classification_report(\n",
    "        y_true=labels_flat, \n",
    "        y_pred=predictions_flat,  \n",
    "        target_names=int2tag.values(),\n",
    "        output_dict=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=int2tag,\n",
    "    label2id=tag2int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='output_dir',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 501/15054 [00:54<26:31,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2972, 'learning_rate': 1.9335724724325762e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1002/15054 [01:49<25:00,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0743, 'learning_rate': 1.8671449448651522e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1501/15054 [02:47<22:57,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0656, 'learning_rate': 1.8007174172977285e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2002/15054 [03:43<24:45,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0588, 'learning_rate': 1.7342898897303046e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2501/15054 [04:41<23:52,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0548, 'learning_rate': 1.6678623621628806e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 3002/15054 [05:38<23:20,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0597, 'learning_rate': 1.6014348345954566e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3501/15054 [06:37<23:34,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0514, 'learning_rate': 1.5350073070280326e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4002/15054 [07:34<20:02,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0487, 'learning_rate': 1.4685797794606086e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 4501/15054 [08:32<19:53,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0474, 'learning_rate': 1.4021522518931846e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5001/15054 [09:30<19:44,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0456, 'learning_rate': 1.3357247243257607e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 33%|███▎      | 5018/15054 [11:16<21:56,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.041264407336711884, 'eval_ADV': {'precision': 0.975478760917919, 'recall': 0.9664606288632088, 'f1-score': 0.9709487553323614, 'support': 18605.0}, 'eval_CONJ': {'precision': 0.9974632610216935, 'recall': 0.9963302752293578, 'f1-score': 0.9968964462123531, 'support': 11445.0}, 'eval_VERB': {'precision': 0.99338722716249, 'recall': 0.9900098290391711, 'f1-score': 0.991695652524796, 'support': 62061.0}, 'eval_ADJ': {'precision': 0.9637307281331094, 'recall': 0.9676005394801374, 'f1-score': 0.965661756833234, 'support': 32624.0}, 'eval_ADP': {'precision': 0.9892465706210795, 'recall': 0.9933321028102072, 'f1-score': 0.9912851271542544, 'support': 43342.0}, 'eval_PRON': {'precision': 0.9957026791109918, 'recall': 0.9907797153738224, 'f1-score': 0.9932350971198929, 'support': 14967.0}, 'eval_X': {'precision': 0.8966666666666666, 'recall': 0.5729499467518637, 'f1-score': 0.6991552956465237, 'support': 939.0}, 'eval_NUM': {'precision': 0.9854413102820746, 'recall': 0.9852620087336245, 'f1-score': 0.9853516513511055, 'support': 5496.0}, 'eval_.': {'precision': 0.9991943407349184, 'recall': 0.999803377966535, 'f1-score': 0.9994987665726445, 'support': 50859.0}, 'eval_PRT': {'precision': 0.976634650905061, 'recall': 0.9728610855565777, 'f1-score': 0.9747442160567794, 'support': 10870.0}, 'eval_NOUN': {'precision': 0.9863339275103981, 'recall': 0.9906057016790387, 'f1-score': 0.9884651993666461, 'support': 110599.0}, 'eval_DET': {'precision': 0.996801523549088, 'recall': 0.9975809407452657, 'f1-score': 0.9971910798466085, 'support': 40925.0}, 'eval_accuracy': 0.9883421232978755, 'eval_macro avg': {'precision': 0.9796734705512907, 'recall': 0.9519646793524008, 'f1-score': 0.9628440870014332, 'support': 402732.0}, 'eval_weighted avg': {'precision': 0.9882710082213098, 'recall': 0.9883421232978755, 'f1-score': 0.9882196313414582, 'support': 402732.0}, 'eval_runtime': 103.8801, 'eval_samples_per_second': 165.595, 'eval_steps_per_second': 20.707, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 5501/15054 [12:16<18:39,  8.53it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0289, 'learning_rate': 1.2692971967583367e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 6001/15054 [13:16<18:19,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0257, 'learning_rate': 1.2028696691909127e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6502/15054 [14:16<16:03,  8.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0262, 'learning_rate': 1.1364421416234887e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7001/15054 [15:16<17:47,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0229, 'learning_rate': 1.070014614056065e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 7502/15054 [16:17<14:19,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0284, 'learning_rate': 1.003587086488641e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8002/15054 [17:17<12:53,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0289, 'learning_rate': 9.371595589212171e-06, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 8502/15054 [18:17<13:03,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0263, 'learning_rate': 8.707320313537931e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 9002/15054 [19:18<11:14,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0262, 'learning_rate': 8.043045037863691e-06, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 9501/15054 [20:18<11:15,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0231, 'learning_rate': 7.378769762189451e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 10002/15054 [21:19<09:06,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.024, 'learning_rate': 6.714494486515213e-06, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 67%|██████▋   | 10036/15054 [22:37<09:30,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0413452573120594, 'eval_ADV': {'precision': 0.9755427463233314, 'recall': 0.9733404998656275, 'f1-score': 0.9744403788204907, 'support': 18605.0}, 'eval_CONJ': {'precision': 0.9969445656918376, 'recall': 0.9978156400174749, 'f1-score': 0.9973799126637555, 'support': 11445.0}, 'eval_VERB': {'precision': 0.9908237714138785, 'recall': 0.993458049338554, 'f1-score': 0.9921391617787862, 'support': 62061.0}, 'eval_ADJ': {'precision': 0.9660717009916094, 'recall': 0.9705431584109858, 'f1-score': 0.9683022676187709, 'support': 32624.0}, 'eval_ADP': {'precision': 0.9915958647049342, 'recall': 0.9936320428222047, 'f1-score': 0.992612909545596, 'support': 43342.0}, 'eval_PRON': {'precision': 0.9941856579562922, 'recall': 0.9939199572392597, 'f1-score': 0.994052789842967, 'support': 14967.0}, 'eval_X': {'precision': 0.9315707620528771, 'recall': 0.6379126730564431, 'f1-score': 0.7572692793931732, 'support': 939.0}, 'eval_NUM': {'precision': 0.9833664798408968, 'recall': 0.9896288209606987, 'f1-score': 0.9864877119796862, 'support': 5496.0}, 'eval_.': {'precision': 0.9994299530202662, 'recall': 0.9997050669498024, 'f1-score': 0.9995674910549286, 'support': 50859.0}, 'eval_PRT': {'precision': 0.9779020722538052, 'recall': 0.9811407543698252, 'f1-score': 0.9795187362233652, 'support': 10870.0}, 'eval_NOUN': {'precision': 0.9902008730460613, 'recall': 0.9885803669110932, 'f1-score': 0.989389956428691, 'support': 110599.0}, 'eval_DET': {'precision': 0.9968496629872032, 'recall': 0.9974098961514967, 'f1-score': 0.9971297008781894, 'support': 40925.0}, 'eval_accuracy': 0.9894694238352055, 'eval_macro avg': {'precision': 0.9828736758569162, 'recall': 0.9597572438411222, 'f1-score': 0.9690241913523666, 'support': 402732.0}, 'eval_weighted avg': {'precision': 0.9894341516239861, 'recall': 0.9894694238352055, 'f1-score': 0.9893864696265574, 'support': 402732.0}, 'eval_runtime': 73.0178, 'eval_samples_per_second': 235.587, 'eval_steps_per_second': 29.459, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 10501/15054 [23:35<09:31,  7.97it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0157, 'learning_rate': 6.0502192108409726e-06, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11001/15054 [24:37<08:14,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0136, 'learning_rate': 5.385943935166733e-06, 'epoch': 2.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 11501/15054 [25:39<07:20,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0139, 'learning_rate': 4.7216686594924946e-06, 'epoch': 2.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 12001/15054 [26:41<05:52,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0127, 'learning_rate': 4.057393383818255e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 12502/15054 [27:42<04:59,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0132, 'learning_rate': 3.3931181081440153e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 13001/15054 [28:46<04:08,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0156, 'learning_rate': 2.728842832469776e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 13501/15054 [29:48<03:39,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0164, 'learning_rate': 2.064567556795536e-06, 'epoch': 2.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14001/15054 [30:51<02:06,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0122, 'learning_rate': 1.4002922811212968e-06, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 14501/15054 [31:54<01:09,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0135, 'learning_rate': 7.360170054470574e-07, 'epoch': 2.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 15001/15054 [32:58<00:06,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0127, 'learning_rate': 7.174172977281786e-08, 'epoch': 2.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 15054/15054 [34:20<00:00,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04342537000775337, 'eval_ADV': {'precision': 0.9765848394928514, 'recall': 0.9729105079279764, 'f1-score': 0.974744211093161, 'support': 18605.0}, 'eval_CONJ': {'precision': 0.997117903930131, 'recall': 0.9975535168195718, 'f1-score': 0.9973356628084734, 'support': 11445.0}, 'eval_VERB': {'precision': 0.9913520117021105, 'recall': 0.9937480865599975, 'f1-score': 0.9925486030642462, 'support': 62061.0}, 'eval_ADJ': {'precision': 0.9690797320855125, 'recall': 0.9712481608631682, 'f1-score': 0.9701627348019779, 'support': 32624.0}, 'eval_ADP': {'precision': 0.992139053459047, 'recall': 0.9929860181809792, 'f1-score': 0.9925623551388937, 'support': 43342.0}, 'eval_PRON': {'precision': 0.9931959175505303, 'recall': 0.9947885347765083, 'f1-score': 0.993991588223513, 'support': 14967.0}, 'eval_X': {'precision': 0.9120111731843575, 'recall': 0.6954206602768903, 'f1-score': 0.7891238670694865, 'support': 939.0}, 'eval_NUM': {'precision': 0.9828890489913544, 'recall': 0.9929039301310044, 'f1-score': 0.9878711078928313, 'support': 5496.0}, 'eval_.': {'precision': 0.9994104237088279, 'recall': 0.9999016889832675, 'f1-score': 0.9996559959898961, 'support': 50859.0}, 'eval_PRT': {'precision': 0.9770957398076042, 'recall': 0.9811407543698252, 'f1-score': 0.979114069313748, 'support': 10870.0}, 'eval_NOUN': {'precision': 0.9906420258113563, 'recall': 0.9897015343719202, 'f1-score': 0.9901715567657049, 'support': 110599.0}, 'eval_DET': {'precision': 0.99740881979075, 'recall': 0.9969945021380574, 'f1-score': 0.9972016179291973, 'support': 40925.0}, 'eval_accuracy': 0.9899759641647548, 'eval_macro avg': {'precision': 0.9815772241262026, 'recall': 0.964941491283264, 'f1-score': 0.9720402808409276, 'support': 402732.0}, 'eval_weighted avg': {'precision': 0.9899355883780958, 'recall': 0.9899759641647548, 'f1-score': 0.9899206958838205, 'support': 402732.0}, 'eval_runtime': 75.555, 'eval_samples_per_second': 227.675, 'eval_steps_per_second': 28.469, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15054/15054 [34:24<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2064.0528, 'train_samples_per_second': 58.339, 'train_steps_per_second': 7.293, 'train_loss': 0.040027938208067514, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15054, training_loss=0.040027938208067514, metrics={'train_runtime': 2064.0528, 'train_samples_per_second': 58.339, 'train_steps_per_second': 7.293, 'train_loss': 0.040027938208067514, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task='token-classification',\n",
    "    model='my_model',\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'NOUN',\n",
       "  'score': 0.99942374,\n",
       "  'index': 1,\n",
       "  'word': 'Need',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.9993299,\n",
       "  'index': 2,\n",
       "  'word': 'For',\n",
       "  'start': 5,\n",
       "  'end': 8},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9998654,\n",
       "  'index': 3,\n",
       "  'word': 'Speed',\n",
       "  'start': 9,\n",
       "  'end': 14},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.63851696,\n",
       "  'index': 4,\n",
       "  'word': ':',\n",
       "  'start': 14,\n",
       "  'end': 15},\n",
       " {'entity': 'ADV',\n",
       "  'score': 0.96408814,\n",
       "  'index': 5,\n",
       "  'word': 'Most',\n",
       "  'start': 16,\n",
       "  'end': 20},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.91693956,\n",
       "  'index': 6,\n",
       "  'word': 'Wanted',\n",
       "  'start': 21,\n",
       "  'end': 27},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.9999552,\n",
       "  'index': 7,\n",
       "  'word': 'is',\n",
       "  'start': 28,\n",
       "  'end': 30},\n",
       " {'entity': 'DET',\n",
       "  'score': 0.9999356,\n",
       "  'index': 8,\n",
       "  'word': 'the',\n",
       "  'start': 31,\n",
       "  'end': 34},\n",
       " {'entity': 'ADJ',\n",
       "  'score': 0.9996879,\n",
       "  'index': 9,\n",
       "  'word': 'best',\n",
       "  'start': 35,\n",
       "  'end': 39},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.997012,\n",
       "  'index': 10,\n",
       "  'word': 'video',\n",
       "  'start': 40,\n",
       "  'end': 45},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9998442,\n",
       "  'index': 11,\n",
       "  'word': 'game',\n",
       "  'start': 46,\n",
       "  'end': 50},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.9999292,\n",
       "  'index': 12,\n",
       "  'word': 'of',\n",
       "  'start': 51,\n",
       "  'end': 53},\n",
       " {'entity': 'PRT',\n",
       "  'score': 0.9994665,\n",
       "  'index': 13,\n",
       "  'word': 'all',\n",
       "  'start': 54,\n",
       "  'end': 57},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99977785,\n",
       "  'index': 14,\n",
       "  'word': 'time',\n",
       "  'start': 58,\n",
       "  'end': 62},\n",
       " {'entity': '.',\n",
       "  'score': 0.99998474,\n",
       "  'index': 15,\n",
       "  'word': '.',\n",
       "  'start': 62,\n",
       "  'end': 63}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Need For Speed: Most Wanted is the best video game of all time.'\n",
    "pipe(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
